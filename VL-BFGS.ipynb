{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast L-BFGS and VL-BFGS on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. MODULE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computation\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "\n",
    "# Random\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "# Perso\n",
    "from common import *\n",
    "from VL_BFGS import *\n",
    "from feature_engineering import *\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DONNÉES SIMULÉES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Vérification de la convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons commencer par vérifier la convergence de nos deux algorithmes en les comparant à l'implémentation Scipy. \n",
    "\n",
    "Pour cela nous simulons un petit jeu de données pour une régression linéaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 1000\n",
    "n = Normal(0, 1)\n",
    "coefs = n.sample(torch.Size([n_features]))\n",
    "\n",
    "lbda = 0.1\n",
    "w0 = torch.zeros(n_features)\n",
    "\n",
    "f = linear_loss\n",
    "f_grad = linear_grad\n",
    "\n",
    "X, y = simulate_data(coefs, n_samples=50000, for_logreg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scipy n'accepte que des array numpy en input, pour cela nous transformons nos torch tensor. Nous utilisons aussi des fonctions codées en numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0_numpy = w0.numpy()\n",
    "\n",
    "f_numpy = linear_loss_numpy\n",
    "f_grad_numpy = linear_grad_numpy\n",
    "\n",
    "X_numpy, y_numpy = X.numpy(), y.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scipy\n",
    "_, f_min_scipy, _ = fmin_l_bfgs_b(f_numpy, w0_numpy, f_grad_numpy, args=(X_numpy, y_numpy, lbda))\n",
    "\n",
    "# Notre implémentation de L-BFGS\n",
    "optimizer = lbfgs(f, f_grad, m=10, vector_free=False, device='cpu')\n",
    "_, f_min, _, _ = optimizer.fit(X, y, w0, lbda)\n",
    "\n",
    "# Notre implémentation de VL-BFGS\n",
    "optimizer_VL = lbfgs(f, f_grad, m=10, vector_free=True, device='cpu')\n",
    "_, f_min_VL, _, _ = optimizer.fit(X, y, w0, lbda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous comparons enfin les loss obtenues par les 3 algorithmes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Scipy: %.4f' % (f_min_scipy))\n",
    "print('L-BFGS: %.4f' % (f_min[-1]))\n",
    "print('VL-BFGS: %.4f' % (f_min_VL[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos deux implémentations convergent donc bien, nous allons donc maintenant pouvoir nous intéresser à leur vitesse de convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_numpy, y_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Convergence et temps de calcul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 200000\n",
    "n_features = 3000\n",
    "\n",
    "n = Normal(0, 1)\n",
    "coefs = n.sample(torch.Size([n_features]))\n",
    "\n",
    "lbda = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Regression linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = simulate_data(coefs, n_samples, for_logreg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = torch.zeros(n_features)\n",
    "f = linear_loss\n",
    "f_grad = linear_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# CPU\n",
    "optimizer = lbfgs(f, f_grad, m=10, vector_free=False, device='cpu')\n",
    "_, objectives_cpu, computing_time_cpu, _ = optimizer.fit(X, y, w0, lbda)\n",
    "\n",
    "# GPU\n",
    "optimizer = lbfgs(f, f_grad, m=10, vector_free=False, device='cuda:0')\n",
    "_, objectives_gpu, computing_time_gpu, _ = optimizer.fit(X, y, w0, lbda)\n",
    "\n",
    "# GPU VL\n",
    "optimizer = lbfgs(f, f_grad, m=10, vector_free=True, device='cuda:0')\n",
    "_, objectives_gpu_vl, computing_time_gpu_vl, _ = optimizer.fit(X, y, w0, lbda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_min = min(objectives_gpu[-1], objectives_cpu[-1], objectives_gpu_vl[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.semilogy(np.linspace(0, computing_time_cpu, len(objectives_cpu)), objectives_cpu - f_min, lw=2, label='CPU')\n",
    "plt.semilogy(np.linspace(0, computing_time_gpu, len(objectives_gpu)), objectives_gpu - f_min, lw=2, label='GPU')\n",
    "plt.semilogy(np.linspace(0, computing_time_gpu_vl, len(objectives_gpu_vl)), \n",
    "             objectives_gpu_vl - f_min, lw=2, label='GPU (vector-free)')\n",
    "plt.yscale('log')\n",
    "plt.title(\"Linear Regression\", fontsize=16)\n",
    "plt.legend(title='Device', loc='best')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Distance to minimum')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.sign(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = torch.zeros(n_features)\n",
    "f = logistic_loss\n",
    "f_grad = logistic_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU\n",
    "optimizer = lbfgs(f, f_grad, m=10, vector_free=False, device='cpu')\n",
    "_, objectives_cpu, computing_time_cpu, _ = optimizer.fit(X, y, w0, lbda)\n",
    "\n",
    "# GPU\n",
    "optimizer = lbfgs(f, f_grad, m=10, vector_free=False, device='cuda:0')\n",
    "_, objectives_gpu, computing_time_gpu, _ = optimizer.fit(X, y, w0, lbda)\n",
    "\n",
    "# GPU VL\n",
    "optimizer = lbfgs(f, f_grad, m=10, vector_free=True, device='cuda:0')\n",
    "_, objectives_gpu_vl, computing_time_gpu_vl, _ = optimizer.fit(X, y, w0, lbda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_min = min(objectives_gpu[-1], objectives_cpu[-1], objectives_gpu_vl[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.semilogy(np.linspace(0, computing_time_cpu, len(objectives_cpu)), objectives_cpu - f_min, lw=2, label='CPU')\n",
    "plt.semilogy(np.linspace(0, computing_time_gpu, len(objectives_gpu)), objectives_gpu - f_min, lw=2, label='GPU')\n",
    "plt.semilogy(np.linspace(0, computing_time_gpu_vl, len(objectives_gpu_vl)), \n",
    "             objectives_gpu_vl - f_min, lw=2, label='GPU (vector-free)')\n",
    "plt.yscale('log')\n",
    "plt.title(\"Logistic Regression\", fontsize=16)\n",
    "plt.legend(title='Device', loc='best')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Distance to minimum')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le temps de calcul est beaucoup moins important sur GPU que sur CPU dans le cas des deux algorithmes considérés.\n",
    "\n",
    "Néanmoins VL-BFGS semble légérement moins rapide que L-BFGS. Cela est normal puisque le premier a d'abord été pensé pour être distribué et scalable du point de vue des features. Il serait donc plus rapide que le second sur un plus grand nombre de GPUs et pour un plus grand nombre de variables (que nous n'avons pu atteindre dû à une ram insuffisante).\n",
    "\n",
    "Dans cette première comparaison nous n'avons pas pris en compte le temps de communication entre le CPU et le GPU, ce qui nuit à la performance globale des algorithmes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Temps d'exécution en fonction de la dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = logistic_loss\n",
    "f_grad = logistic_grad\n",
    "\n",
    "n_features = np.arange(200, 10000, 200)\n",
    "n_samples = 50000\n",
    "\n",
    "cpu_time, gpu_time, gpu_vl_time = [], [], []\n",
    "\n",
    "for n_feature in n_features:\n",
    "    \n",
    "    n = Normal(0, 1)\n",
    "    coefs = n.sample(torch.Size([n_feature]))\n",
    "    w0 = torch.zeros(n_feature)\n",
    "    X, y = simulate_data(coefs, n_samples=50000, for_logreg=True)\n",
    "    \n",
    "    # CPU\n",
    "    optimizer = lbfgs(f, f_grad, m=10, vector_free=False, device='cpu')\n",
    "    _, _, computing_time_cpu, com_time_cpu = optimizer.fit(X, y, w0, lbda)\n",
    "    \n",
    "    # GPU\n",
    "    optimizer = lbfgs(f, f_grad, m=10, vector_free=False, device='cuda:0')\n",
    "    _, _, computing_time_gpu, com_time_gpu = optimizer.fit(X, y, w0, lbda)\n",
    "    \n",
    "    # GPU VL\n",
    "    optimizer = lbfgs(f, f_grad, m=10, vector_free=True, device='cuda:0')\n",
    "    _, _, computing_time_gpu_vl, com_time_gpu_vl = optimizer.fit(X, y, w0, lbda)\n",
    "    \n",
    "    cpu_time.append(computing_time_cpu + com_time_cpu)\n",
    "    gpu_time.append(computing_time_gpu + com_time_gpu)\n",
    "    gpu_vl_time.append(computing_time_gpu_vl + com_time_gpu_vl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('Runtime comparison', fontsize=16)\n",
    "plt.plot(n_features, cpu_time, label='CPU')\n",
    "plt.plot(n_features, gpu_time, label='GPU')\n",
    "plt.plot(n_features, gpu_vl_time, label='GPU (vector-free)')\n",
    "plt.legend(title='Device', loc='best')\n",
    "plt.ylabel('Runtime (s)')\n",
    "plt.xlabel('Dimension')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Comparaison détaillée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie nous allons regarder précisément le temps de communication entre le CPU et le GPU (le temps nécessaire pour que les données soient envoyées au GPU) et le temps de calcul. Nous allons exécuter chaque algorithme 10x, et ce pour plusieurs datasets (toujours simulés) afin d'obtenir des statistiques plus représentatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python simulation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DONNÉES RÉELLES "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie nous allons appliquer nos deux algorithmes à un jeu de données réelles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le dataset comporte 33 variables, lesquelles sont différentes mesures physiques concernant l'activité solaire. Il y a environ 500000 observations, chacune représentant un instant t. Il s'agit de prédire, pour chaque instant t, la présence d'une tempête solaire.\n",
    "\n",
    "Il s'agit d'une tache de classification pour laquelle nous pouvons utiliser une regression logistique. Cependant ce modèle ne permet pas naturellement d'assimiler la relation temporelle entre les différentes observations. Afin de palier ce défaut nous allons faire un important feature engineering afin de créer de nombreuses variables \"temporelles\". Nous allons par exemple créer une variable valant, pour l'observation t et le variable v, la moyenne de la variable v sur l'intervalle [t-1h, t]. \n",
    "\n",
    "En faisant ça pour toutes les variables, en utilisant différentes mesures (moyenne, médiane, variance, minimum, maximum) et sur plusieurs intervalles temporels (1h, 3h, 5h, 10h, 20h) nous obtenons un total de 1749."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('>> Loading data')\n",
    "X = np.load('data/data_train.npy')\n",
    "y = pd.read_csv('data/labels_train.csv')\n",
    "\n",
    "print('>> To tensor')\n",
    "X = torch.tensor(X, dtype=torch.float)\n",
    "y = torch.tensor(y.values, dtype=torch.float).squeeze()\n",
    "\n",
    "print('>> Training on %2d samples and %2d features' % X.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "lbda = 0.1\n",
    "\n",
    "# Initialization\n",
    "w0 = torch.zeros(X.size(1))\n",
    "f = logistic_loss\n",
    "f_grad = logistic_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU\n",
    "optimizer = lbfgs(f, f_grad, m=10, vector_free=False, device='cpu')\n",
    "_, objectives_cpu, computing_time_cpu, _ = optimizer.fit(X, y, w0, lbda)\n",
    "\n",
    "# GPU\n",
    "optimizer = lbfgs(f, f_grad, m=10, vector_free=False, device='cuda:0')\n",
    "_, objectives_gpu, computing_time_gpu, _ = optimizer.fit(X, y, w0, lbda)\n",
    "\n",
    "# GPU VL\n",
    "optimizer = lbfgs(f, f_grad, m=10, vector_free=True, device='cuda:0')\n",
    "_, objectives_gpu_vl, computing_time_gpu_vl, _ = optimizer.fit(X, y, w0, lbda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_min = min(objectives_gpu[-1], objectives_cpu[-1], objectives_gpu_vl[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.semilogy(np.linspace(0, computing_time_cpu, len(objectives_cpu)), objectives_cpu - f_min, lw=2, label='CPU')\n",
    "plt.semilogy(np.linspace(0, computing_time_gpu, len(objectives_gpu)), objectives_gpu - f_min, lw=2, label='GPU')\n",
    "plt.semilogy(np.linspace(0, computing_time_gpu_vl, len(objectives_gpu_vl)), \n",
    "             objectives_gpu_vl - f_min, lw=2, label='GPU (vector-free)')\n",
    "plt.yscale('log')\n",
    "plt.title(\"Real data\", fontsize=16)\n",
    "plt.legend(title='Device', loc='best')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Distance to minimum')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CONCLUSION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
